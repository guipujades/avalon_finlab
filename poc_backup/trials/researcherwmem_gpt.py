import os
import asyncio
from datetime import datetime
from dotenv import load_dotenv
from tavily import AsyncTavilyClient
from llama_index.llms.openai import OpenAI
from llama_index.core.agent import FunctionCallingAgentWorker
from llama_index.core.agent import AgentRunner
from llama_index.core.tools import FunctionTool
from llama_index.core.memory import ChatMemoryBuffer

def get_current_date():
    """Retorna a data atual formatada em portuguÃªs"""
    now = datetime.now()
    months = [
        "janeiro", "fevereiro", "marÃ§o", "abril", "maio", "junho",
        "julho", "agosto", "setembro", "outubro", "novembro", "dezembro"
    ]
    day = now.day
    month = months[now.month - 1]
    year = now.year
    weekdays = ["segunda", "terÃ§a", "quarta", "quinta", "sexta", "sÃ¡bado", "domingo"]
    weekday = weekdays[now.weekday()]
   
    return f"{weekday}-feira, {day} de {month} de {year}"

async def search_web(query: str) -> str:
    """
    Pesquisa informaÃ§Ãµes na web usando Tavily.
    
    Args:
        query: A consulta de pesquisa
        
    Returns:
        Resultados da pesquisa formatados como string
    """
    client = AsyncTavilyClient(api_key=tavily_api_key)
    results = await client.search(query)
    return str(results)

async def main():
    global tavily_api_key, api_key
   
    load_dotenv()
    tavily_api_key = os.getenv('TAVILY_KEY')
    api_key = os.getenv('OPENAI_API_KEY')
   
    # Inicializa o LLM
    llm = OpenAI(model="gpt-4o-mini", api_key=api_key)
    
    # Cria a ferramenta de pesquisa
    search_tool = FunctionTool.from_defaults(
        fn=search_web,
        name="search_web",
        description="Pesquisa informaÃ§Ãµes na web sobre economia, finanÃ§as e notÃ­cias atuais"
    )
    
    # ObtÃ©m a data atual
    get_date = get_current_date()
    
    # Prompt do sistema
    system_prompt = f"""VocÃª Ã© um assistente de pesquisa inteligente com acesso Ã  web.
VocÃª possui conhecimentos de economia e finanÃ§as e estÃ¡ sempre atualizado com as noticias do Brasil e do mundo.
Sua pesquisa deve ser sempre feita tendo por base a data da pergunta: {get_date}.
Dessa forma, caso a pergunta envolva algum dado, deve ser feita com a informaÃ§Ã£o mais atualizada possÃ­vel.

OBJETIVO:
Ajudar com pesquisas envolvendo economia e finanÃ§as na internet de forma eficiente e precisa.

PROCESSO:
1. Analise a pergunta do usuÃ¡rio considerando o contexto de conversas anteriores
2. Caso a pergunta nÃ£o esteja clara, faÃ§a perguntas de esclarecimento
3. Use search_web para buscar informaÃ§Ãµes relevantes quando necessÃ¡rio
4. ForneÃ§a respostas claras baseadas nos resultados e no contexto da conversa

REGRAS:
- Seja especÃ­fico e direto
- Use fontes confiÃ¡veis
- Inclua informaÃ§Ãµes atualizadas
- Mantenha continuidade com as conversas anteriores
- Referencie informaÃ§Ãµes discutidas anteriormente quando relevante
- Lembre-se do contexto e das preferÃªncias do usuÃ¡rio ao longo da conversa"""
    
    # Cria a memÃ³ria de chat
    memory = ChatMemoryBuffer.from_defaults(
        llm=llm,
        token_limit=3000,  # Limite de tokens para o contexto
    )
    
    # Cria o agente worker
    agent_worker = FunctionCallingAgentWorker.from_tools(
        tools=[search_tool],
        llm=llm,
        system_prompt=system_prompt,
        verbose=True
    )
    
    # Cria o AgentRunner com memÃ³ria
    agent = AgentRunner(
        agent_worker,
        memory=memory,
    )
    
    print("ğŸ¤– Agente de pesquisa iniciado com memÃ³ria de contexto!")
    print("ğŸ“ Comandos disponÃ­veis:")
    print("   - 'sair' para encerrar")
    print("   - 'limpar' para limpar o histÃ³rico")
    print("   - 'resumo' para ver um resumo da conversa")
    print("-" * 50)
    
    while True:
        try:
            user_prompt = input("\nğŸ’¬ Sua pergunta: ")
            
            # Comandos especiais
            if user_prompt.lower() in ['sair', 'exit', 'quit']:
                print("ğŸ‘‹ Encerrando o agente. AtÃ© logo!")
                break
            
            if user_prompt.lower() == 'limpar':
                # Reinicia a memÃ³ria
                memory.reset()
                print("ğŸ—‘ï¸  HistÃ³rico de conversas limpo!")
                continue
            
            if user_prompt.lower() == 'resumo':
                # ObtÃ©m o resumo da conversa
                chat_history = memory.get()
                if chat_history:
                    print("\nğŸ“‹ === RESUMO DA CONVERSA ===")
                    for i, msg in enumerate(chat_history):
                        role = "VocÃª" if msg.role == "user" else "Assistente"
                        content_preview = msg.content[:150] + "..." if len(msg.content) > 150 else msg.content
                        print(f"\n[{i+1}] {role}: {content_preview}")
                else:
                    print("ğŸ“­ HistÃ³rico vazio.")
                continue
            
            if not user_prompt.strip():
                continue
            
            # Executa a query com streaming
            print("\nğŸ” Processando...")
            response = await agent.achat(user_prompt)
            
            # Exibe a resposta
            print(f"\nğŸ¤– {response}")
           
        except KeyboardInterrupt:
            print("\n\nâš ï¸  Interrompido pelo usuÃ¡rio.")
            break
        except Exception as e:
            print(f"âŒ Erro: {e}")
            print("Tente novamente ou digite 'sair' para encerrar.")

if __name__ == "__main__":
    asyncio.run(main())